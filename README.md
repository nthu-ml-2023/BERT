# BERT (Bidirectional Encoder Representations from Transformers)

## Introduction
This is the code for BERT model. 
We use the code to classify the sentimental class of input sentences from chatroom analysis.
After preprocess and tokenize the data, we use them to fine-tune the pre-trained Chinese BERT model and make prediction (positive/ negative) for input sentences from chatroom system. Next, the result will be shown at the chatroom.

## Usage
1. Change the file path of dataset.
2. Run the codes. (You can skip some of the code blocks if there are not any problems of memory or CPU/GPU resources.)
3. Analysis (Bertviz)
